{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc718d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a59770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def softmax(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp / sum(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8026d4",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "a tiny dataset with 100 examples of SMSs with label either it is spam or ham. it is preprocessed version so that we can much directed on RNN.\n",
    "<br>This 100 examples collected from [Kaggle - SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset), full dataset credits goes to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7095bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[[[1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>spam</td>\n",
       "      <td>TheMob&gt; Check out our newest selection of cont...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>spam</td>\n",
       "      <td>Think ur smart ? Win å£200 this week in our we...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>spam</td>\n",
       "      <td>December only! Had your mobile 11mths+? You ar...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>spam</td>\n",
       "      <td>Call Germany for only 1 pence per minute! Call...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>spam</td>\n",
       "      <td>Valentines Day Special! Win over å£1000 in our...</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2    ham  U dun say so early hor... U c already then say...   \n",
       "3    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4    ham  Even my brother is not like to speak with me. ...   \n",
       "..   ...                                                ...   \n",
       "95  spam  TheMob> Check out our newest selection of cont...   \n",
       "96  spam  Think ur smart ? Win å£200 this week in our we...   \n",
       "97  spam  December only! Had your mobile 11mths+? You ar...   \n",
       "98  spam  Call Germany for only 1 pence per minute! Call...   \n",
       "99  spam  Valentines Day Special! Win over å£1000 in our...   \n",
       "\n",
       "                                         encoded_text  ham  spam  \n",
       "0   [[[1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    1     0  \n",
       "1   [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    1     0  \n",
       "2   [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    1     0  \n",
       "3   [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    1     0  \n",
       "4   [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    1     0  \n",
       "..                                                ...  ...   ...  \n",
       "95  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    0     1  \n",
       "96  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    0     1  \n",
       "97  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    0     1  \n",
       "98  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    0     1  \n",
       "99  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...    0     1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"datasets/preprocessed-spam-sms.pkl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04c8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train and test sets\n",
    "# train set with 90 samples and test set with 10 samples.\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train_data = data.head(90).copy().reset_index(drop=True)\n",
    "test_data = data.tail(10).copy().reset_index(drop=True)\n",
    "\n",
    "X_train = np.array(train_data[\"encoded_text\"])\n",
    "y_train = np.array(train_data[[\"ham\",\"spam\"]])\n",
    "\n",
    "test_text = test_data[\"text\"]\n",
    "X_test = np.array(test_data[\"encoded_text\"])\n",
    "y_test = np.array(test_data[[\"ham\",\"spam\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90128721",
   "metadata": {},
   "source": [
    "### Model: Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are a type of neural network designed for processing sequences of data. They use recurrent connections to maintain a hidden state, allowing them to capture dependencies within sequences. RNNs are used in tasks like natural language processing and time series analysis.\n",
    "\n",
    "The architecture of a Recurrent Neural Network (RNN) consists of:\n",
    "\n",
    "1. Input Layer: Receives sequential data.\n",
    "\n",
    "2. Hidden State: Maintains information from previous time steps, allowing the network to capture temporal dependencies.\n",
    "\n",
    "3. Weight Sharing: Reuses the same set of weights and biases across time steps.\n",
    "\n",
    "4. Output Layer: Produces predictions or representations based on the processed sequence.\n",
    "\n",
    "5. Training: RNNs are trained using backpropagation through time (BPTT), where gradients are computed and weights are updated.\n",
    "\n",
    "RNNs can be further extended with more advanced variants like LSTM and GRU to address issues like vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b65902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    input_size = 750\n",
    "    output_size = 2\n",
    "    hidden_size = 90\n",
    "    lr = 0.001\n",
    "    cr = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = { 'input-to-hidden' :np.random.randn(self.hidden_size, self.input_size), \n",
    "                         'hidden-to-hidden':np.random.randn(self.hidden_size, self.hidden_size),\n",
    "                         'hidden-to-output':np.random.randn(self.output_size, self.hidden_size) }\n",
    "        self.biases = { 'hidden':np.zeros((self.hidden_size, 1)),\n",
    "                        'output':np.zeros((self.output_size, 1)) }\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden_activations = np.zeros((self.hidden_size, 1))\n",
    "        # saving to history\n",
    "        self.last_inputs = inputs\n",
    "        self.states = { 0: hidden_activations }\n",
    "        # passing each word into input and hidden layers\n",
    "        for i, x in enumerate(inputs):\n",
    "            input_z = self.weights['input-to-hidden'].dot(x)\n",
    "            hidden_activations = np.tanh( input_z + self.weights['hidden-to-hidden'].dot(hidden_activations) + self.biases['hidden'])\n",
    "            self.states[i + 1] = hidden_activations\n",
    "        # passing through output layer\n",
    "        outputs = softmax(self.weights['hidden-to-output'].dot(hidden_activations) + self.biases['output'])\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, error):\n",
    "        \n",
    "        # intializing new weights and biases for input layer, hidden layer\n",
    "        nw_input_to_hidden = np.zeros((self.hidden_size, self.input_size))\n",
    "        nw_hidden_to_hidden = np.zeros((self.hidden_size, self.hidden_size))\n",
    "        nb_hidden = np.zeros((self.hidden_size, 1))\n",
    "        # Calculating outputlayer new weights and biases\n",
    "        nw_hidden_to_output = error.dot(self.states[len(self.last_inputs)].T)\n",
    "        nb_output = error\n",
    "        # calculating hidden layer error with respect to output layer\n",
    "        hidden_error = self.weights['hidden-to-output'].T.dot(error)\n",
    "        \n",
    "        # calculating new weights for hidden layers\n",
    "        for ti in reversed(range(len(self.last_inputs))):\n",
    "            \n",
    "            temp = ((1 - self.states[ti + 1] ** 2) * hidden_error)\n",
    "            # Calculating new hiddenlayer bias\n",
    "            nb_hidden += temp\n",
    "            # Calculating new hidden-to-hidden weights\n",
    "            nw_hidden_to_hidden += temp.dot(self.states[ti].T)\n",
    "            # Calculating new input-to-hidden weights\n",
    "            nw_input_to_hidden += temp.dot(self.last_inputs[ti].T)\n",
    "            # updating hidden layer error\n",
    "            hidden_error = self.weights['hidden-to-hidden'].dot(temp)\n",
    "        \n",
    "        # Clipping all gradients\n",
    "        for d in [nw_input_to_hidden, nw_hidden_to_hidden, nw_hidden_to_output, nb_hidden, nb_output]:\n",
    "            np.clip(d, -self.cr, self.cr, out=d)\n",
    "        \n",
    "        # updating weights and biases\n",
    "        self.weights['hidden-to-output'] -= self.lr * nw_hidden_to_output\n",
    "        self.weights['hidden-to-hidden'] -= self.lr * nw_hidden_to_hidden\n",
    "        self.weights['input-to-hidden'] -= self.lr * nw_input_to_hidden\n",
    "        self.biases['output'] -= self.lr * nb_output\n",
    "        self.biases['hidden'] -= self.lr * nb_hidden\n",
    "    \n",
    "    def fit(self, X, y, epochs=1):\n",
    "        \n",
    "        batch_size = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            batch_loss = []\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                # forward\n",
    "                out = self.forward(X[i])\n",
    "                # loss\n",
    "                batch_loss.append(-np.log(out[np.argmax(y[i])])[0])\n",
    "                # backward\n",
    "                out[np.argmax(y[i])] -= 1\n",
    "                self.backward(out)\n",
    "            \n",
    "            if epoch%100==0: print('epoch ',(epoch+1),'\\t... loss: ',sum(batch_loss)/batch_size)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        hidden_activations = np.zeros((self.hidden_size, 1))\n",
    "        for _ , x in enumerate(inputs):\n",
    "            hidden_activations = np.tanh( self.weights['input-to-hidden'].dot(x) + self.weights['hidden-to-hidden'].dot(hidden_activations) + self.biases['hidden'])\n",
    "        \n",
    "        return softmax(self.weights['hidden-to-output'].dot(hidden_activations) + self.biases['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7610e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test_prediction(model):\n",
    "    label_names = ['ham', 'spam']\n",
    "    tag = np.random.randint(0,10)\n",
    "    print('\\nTest prediction:',\n",
    "          '\\nSMS: ',test_text[tag] ,\n",
    "          '\\nactual    : ',label_names[np.argmax(y_test[tag])],\n",
    "          '\\nprediction: ',label_names[np.argmax(model.predict(X_test[tag]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2ad0c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d101c995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 \t... loss:  2.828153920636377\n",
      "epoch  101 \t... loss:  2.0558761333003606\n",
      "epoch  201 \t... loss:  0.9623349563030441\n",
      "epoch  301 \t... loss:  0.6977277797023781\n",
      "epoch  401 \t... loss:  0.6664936161802586\n",
      "epoch  501 \t... loss:  0.6343851476026469\n",
      "epoch  601 \t... loss:  0.5513082546863726\n",
      "epoch  701 \t... loss:  0.5600874677127684\n",
      "epoch  801 \t... loss:  0.5985504138889881\n",
      "epoch  901 \t... loss:  0.5515569019164757\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.fit(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d293e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test prediction: \n",
      "SMS:  Todays Voda numbers ending 7548 are selected to receive a $350 award. If you have a match please call 08712300220 quoting claim code 4041 standard rates app \n",
      "actual    :  spam \n",
      "prediction:  spam\n"
     ]
    }
   ],
   "source": [
    "random_test_prediction(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e86bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test prediction: \n",
      "SMS:  So Ì_ pay first lar... Then when is da stock comin... \n",
      "actual    :  ham \n",
      "prediction:  ham\n"
     ]
    }
   ],
   "source": [
    "random_test_prediction(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771a00c",
   "metadata": {},
   "source": [
    "#### Looks like model learned somethig!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
